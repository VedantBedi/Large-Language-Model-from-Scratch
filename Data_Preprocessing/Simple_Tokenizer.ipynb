{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e658a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8474cbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped non-text file: .DS_Store\n",
      "Number of files read: 1\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"data\"\n",
    "raw_texts = []\n",
    "for filename in os.listdir(data_folder):\n",
    "    file_path = os.path.join(data_folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw_texts.append(f.read())\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Skipped non-text file: {filename}\")\n",
    "\n",
    "print(\"Number of files read:\", len(raw_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220dbe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in raw text: 275153\n",
      "Sample of raw text: {\\rtf1\\ansi\\ansicpg1252\\cocoartf2822\n",
      "\\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 \n"
     ]
    }
   ],
   "source": [
    "raw_text = \"\\n\".join(raw_texts)\n",
    "print(\"Total characters in raw text:\", len(raw_text))\n",
    "print(\"Sample of raw text:\", raw_text[:100]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "350ba6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in the text: ['Hi,', 'my', 'name', 'is', 'John.', 'I', 'love', 'programming', 'in', 'Python!']\n",
      "Words in the text (punctuation removed): ['Hi', ',', '', ' ', 'my', ' ', 'name', ' ', 'is', ' ', 'John', '.', '', ' ', 'I', ' ', 'love', ' ', 'programming', ' ', 'in', ' ', 'Python!']\n"
     ]
    }
   ],
   "source": [
    "# Now we have multiple options to process the raw text. The first is to split it into words.\n",
    "import re \n",
    "\n",
    "text = \"Hi, my name is John. I love programming in Python!\"\n",
    "result = re.split(r'\\s', text) # splits on whitespaces\n",
    "\n",
    "print(\"Words in the text:\", result)\n",
    "\n",
    "result = re.split(r'([,.]|\\s)', text)  # splits on whitespaces and punctuation\n",
    "print(\"Words in the text (punctuation removed):\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2469bbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in the text (whitespaces removed): ['Hi', ',', 'my', 'name', 'is', 'John', '.', 'I', 'love', 'programming', 'in', 'Python!']\n"
     ]
    }
   ],
   "source": [
    "# Now comes the issue of removing whitespaces, to save memory.\n",
    "result = [word for word in result if word.strip()]\n",
    "print(\"Words in the text (whitespaces removed):\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85408965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf2822', '\\\\cocoatextscaling0\\\\cocoaplatform0{\\\\fonttbl\\\\f0\\\\fswiss\\\\fcharset0', 'Helvetica', ';', '}', '{\\\\colortbl', ';', '\\\\red255\\\\green255\\\\blue255', ';', '}', ';', ';', '}', '\\\\margl1440\\\\margr1440\\\\vieww11520\\\\viewh8400\\\\viewkind0', '\\\\pard\\\\tx720\\\\tx1440\\\\tx2160\\\\tx2880\\\\tx3600\\\\tx4320\\\\tx5040\\\\tx5760\\\\tx6480\\\\tx7200\\\\tx7920\\\\tx8640\\\\pardirnatural\\\\partightenfactor0', '\\\\f0\\\\fs24', '\\\\cf0', 'Chapter', '1\\\\', '\\\\', '\\\\', '\\\\', 'In', 'my', 'younger', 'and', 'more', 'vulnerable', 'years', 'my']\n"
     ]
    }
   ],
   "source": [
    "# There are many ways to process the text, but we will use this one for now.\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "# Remove tokens that are numbers or contain only *\n",
    "preprocessed = [item for item in preprocessed if not ('*' in item or item.isdigit())]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3385650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64491 tokens in the preprocessed text.\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed), \"tokens in the preprocessed text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0b5f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 7146\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "121d5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff95c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "('$3', 2)\n",
      "('$5', 3)\n",
      "(\"'\", 4)\n",
      "('(', 5)\n",
      "(')', 6)\n",
      "(',', 7)\n",
      "('--', 8)\n",
      "('.', 9)\n",
      "('00-6', 10)\n",
      "('00-9', 11)\n",
      "('12th', 12)\n",
      "('15-6', 13)\n",
      "('15-8', 14)\n",
      "('158th', 15)\n",
      "('1\\\\', 16)\n",
      "('2\\\\', 17)\n",
      "('30-4', 18)\n",
      "('30-5', 19)\n",
      "('3\\\\', 20)\n",
      "('4\\\\', 21)\n",
      "('5\\\\', 22)\n",
      "('5th', 23)\n",
      "('6\\\\', 24)\n",
      "('7\\\\', 25)\n",
      "('8\\\\', 26)\n",
      "('9\\\\', 27)\n",
      "(':', 28)\n",
      "(';', 29)\n",
      "('?', 30)\n",
      "('A', 31)\n",
      "('AIN', 32)\n",
      "('AND', 33)\n",
      "('ANYbody', 34)\n",
      "('A\\\\', 35)\n",
      "('About', 36)\n",
      "('Abrams', 37)\n",
      "('Abruptly', 38)\n",
      "('Absolutely', 39)\n",
      "('Across', 40)\n",
      "('Adam', 41)\n",
      "('Adriatic\\\\', 42)\n",
      "('After', 43)\n",
      "('After\\\\', 44)\n",
      "('Afterward', 45)\n",
      "('Again', 46)\n",
      "('Ah', 47)\n",
      "('Ah-h-h', 48)\n",
      "('Ahead', 49)\n",
      "('Albany', 50)\n",
      "('Albrucksburger', 51)\n",
      "('All', 52)\n",
      "('All\\\\', 53)\n",
      "('Allied', 54)\n",
      "('Almost', 55)\n",
      "('Already', 56)\n",
      "('Also', 57)\n",
      "('Amen', 58)\n",
      "('America', 59)\n",
      "('American', 60)\n",
      "('American\\\\', 61)\n",
      "('Americans', 62)\n",
      "('Amid', 63)\n",
      "('Among', 64)\n",
      "('An', 65)\n",
      "('And', 66)\n",
      "('And\\\\', 67)\n",
      "('Angry', 68)\n",
      "('Another', 69)\n",
      "('Antoinette', 70)\n",
      "('Any', 71)\n",
      "('Anyhow', 72)\n",
      "('Anything', 73)\n",
      "('Anywhere', 74)\n",
      "('April', 75)\n",
      "('Araby', 76)\n",
      "('Ardita', 77)\n",
      "('Are', 78)\n",
      "('Aren', 79)\n",
      "('Argonne', 80)\n",
      "('Armistice', 81)\n",
      "('Armistice\\\\', 82)\n",
      "('Arthur', 83)\n",
      "('As', 84)\n",
      "('Asa', 85)\n",
      "('Asheville', 86)\n",
      "('Ask', 87)\n",
      "('Associated', 88)\n",
      "('Astoria', 89)\n",
      "('At', 90)\n",
      "('Atlantic', 91)\n",
      "('Auerbach', 92)\n",
      "('August', 93)\n",
      "('August\\\\', 94)\n",
      "('Aunt', 95)\n",
      "('Australia\\\\', 96)\n",
      "('Auto', 97)\n",
      "('Avenue', 98)\n",
      "('Aware', 99)\n",
      "('Awful', 100)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a3153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item for item in preprocessed if not ('*' in item or item.isdigit())]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bbbe3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[483, 4485, 7136, 1239, 4424, 6835, 7120, 4485, 2972, 3263, 4305, 5916, 1145, 6367, 477, 4, 6781, 1495, 6609, 4733, 3746, 4485, 4368, 2824, 5791, 9]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b65597c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In my younger and more vulnerable years my father gave me some advice that I' ve been turning over in my mind ever since.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b55ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This causes an issue when there is a word that is not in the vocabulary. Now we will add a special token for unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96172792",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|unk|>\",\"<|endoftext|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e2c64e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7148"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aff1471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\\\\colortbl', 7143)\n",
      "('{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf2822', 7144)\n",
      "('}', 7145)\n",
      "('<|unk|>', 7146)\n",
      "('<|endoftext|>', 7147)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfbb0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item for item in preprocessed if not ('*' in item or item.isdigit())]\n",
    "        ids = [self.str_to_int.get(s, self.str_to_int[\"<|unk|>\"]) for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str.get(i, \"<|unk|>\") for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18d0607e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.<|endoftext|> This is a test sentence with an unknownword that is not in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\"\n",
    "text2 = \"This is a test sentence with an unknownword that is not in the vocabulary.\"\n",
    "text = \"<|endoftext|> \".join([text1, text2])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98677ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[483, 4485, 7136, 1239, 4424, 6835, 7120, 4485, 2972, 3263, 4305, 5916, 1145, 6367, 477, 4, 6781, 1495, 6609, 4733, 3746, 4485, 4368, 2824, 5791, 9, 7147, 914, 3911, 1049, 7146, 5629, 7057, 1232, 7146, 6367, 3911, 4582, 3746, 6370, 7146, 9]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74a02db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my younger and more vulnerable years my father gave me some advice that I' ve been turning over in my mind ever since. <|endoftext|> This is a <|unk|> sentence with an <|unk|> that is not in the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.decode(ids)    \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5922be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
